{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa25abc",
   "metadata": {},
   "source": [
    "# Sephora Customer Segmentation (Refined)\n",
    "\n",
    "This notebook performs customer segmentation based on processed customer and product data. The primary goal is to group customers into distinct clusters using relevant features and output a CSV file mapping each `client_id` to a `cluster_id`.\n",
    "\n",
    "**Key changes in this version:**\n",
    "* Loads data from `data/processed/reviews.csv` and `data/processed/skincare_product_info.csv`.\n",
    "* Uses the Silhouette method for determining the optimal number of clusters.\n",
    "* Emphasizes careful feature engineering and selection for customer segmentation based on provided column names.\n",
    "\n",
    "The process involves:\n",
    "1.  Loading pre-processed data.\n",
    "2.  Merging the processed data.\n",
    "3.  Engineering features that describe customer behavior and preferences.\n",
    "4.  Determining optimal cluster count using Silhouette analysis.\n",
    "5.  Applying K-Means clustering.\n",
    "6.  Analyzing cluster characteristics.\n",
    "7.  Exporting the segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ee7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import silhouette_score # For Silhouette analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the base path for PROCESSED data\n",
    "PROCESSED_DATA_PATH = \"../data/processed/\"\n",
    "\n",
    "print(\"Libraries imported and PROCESSED_DATA_PATH set to:\", PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d10eb4",
   "metadata": {},
   "source": [
    "## Cell 3: Load Processed Data & Initial Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e88bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import silhouette_score # For Silhouette analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the base path for PROCESSED data\n",
    "PROCESSED_DATA_PATH = \"../data/processed/\"\n",
    "\n",
    "print(\"Libraries imported and PROCESSED_DATA_PATH set to:\", PROCESSED_DATA_PATH)\n",
    "# This cell loads processed reviews.csv and skincare_product_info.csv, then merges them.\n",
    "# The output of this cell will be `merged_df`.\n",
    "\n",
    "print(f\"Looking for processed data in: {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# Define file names\n",
    "processed_reviews_file = \"reviews.csv\"\n",
    "processed_products_file = \"skincare_product_info.csv\"\n",
    "\n",
    "reviews_csv_path = os.path.join(PROCESSED_DATA_PATH, processed_reviews_file)\n",
    "products_csv_path = os.path.join(PROCESSED_DATA_PATH, processed_products_file)\n",
    "\n",
    "# Load processed reviews data\n",
    "try:\n",
    "    reviews_df = pd.read_csv(reviews_csv_path)\n",
    "    print(f\"Successfully loaded {processed_reviews_file}. Shape: {reviews_df.shape}\")\n",
    "    # print(\"Columns in processed reviews_df:\", reviews_df.columns.tolist())\n",
    "    # print(\"\\nProcessed Reviews DataFrame Sample:\")\n",
    "    # print(reviews_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {reviews_csv_path} not found.\")\n",
    "    reviews_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading {processed_reviews_file}: {e}\")\n",
    "    reviews_df = pd.DataFrame()\n",
    "\n",
    "# Load processed product information\n",
    "try:\n",
    "    products_df = pd.read_csv(products_csv_path)\n",
    "    print(f\"\\nSuccessfully loaded {processed_products_file}. Shape: {products_df.shape}\")\n",
    "    # print(\"Columns in processed skincare_product_info_df (products_df):\", products_df.columns.tolist())\n",
    "    # print(\"\\nProcessed Products DataFrame Sample:\")\n",
    "    # print(products_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {products_csv_path} not found.\")\n",
    "    products_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading {processed_products_file}: {e}\")\n",
    "    products_df = pd.DataFrame()\n",
    "\n",
    "# --- Proceed with merging if both DataFrames are loaded ---\n",
    "if products_df.empty or reviews_df.empty:\n",
    "    print(\"\\nCannot proceed with merging as one or both processed dataframes are missing/empty.\")\n",
    "    merged_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"\\n--- Starting Data Preprocessing & Merge of Processed Files ---\")\n",
    "    \n",
    "    if 'product_id' not in reviews_df.columns:\n",
    "        print(\"Error: 'product_id' column missing in processed reviews.csv.\")\n",
    "    if 'product_id' not in products_df.columns:\n",
    "        print(\"Error: 'product_id' column missing in processed skincare_product_info.csv.\")\n",
    "\n",
    "    if 'product_id' in reviews_df.columns and 'product_id' in products_df.columns:\n",
    "        products_df['product_id'] = products_df['product_id'].astype(str)\n",
    "        reviews_df['product_id'] = reviews_df['product_id'].astype(str)\n",
    "\n",
    "        client_id_col_in_reviews = 'author_id' \n",
    "        if client_id_col_in_reviews not in reviews_df.columns:\n",
    "            print(f\"Error: Client identifier column '{client_id_col_in_reviews}' not found in reviews.csv.\")\n",
    "            merged_df = pd.DataFrame() \n",
    "        else:\n",
    "            reviews_df.dropna(subset=[client_id_col_in_reviews, 'product_id'], inplace=True)\n",
    "            print(f\"Reviews_df shape after dropping NA {client_id_col_in_reviews}/product_id: {reviews_df.shape}\")\n",
    "            \n",
    "            merged_df = pd.merge(reviews_df, products_df, on=\"product_id\", how=\"left\", suffixes=('_review', '_product'))\n",
    "            print(f\"\\nMerged data shape: {merged_df.shape}\")\n",
    "            # print(\"Columns in merged_df after merge:\", merged_df.columns.tolist())\n",
    "            \n",
    "            discount_column_from_product_info = 'discount_usd_product' \n",
    "            if discount_column_from_product_info in merged_df.columns:\n",
    "                merged_df[discount_column_from_product_info] = pd.to_numeric(merged_df[discount_column_from_product_info], errors='coerce')\n",
    "                merged_df['is_on_sale'] = merged_df[discount_column_from_product_info] > 0\n",
    "                print(f\"Created 'is_on_sale' based on '{discount_column_from_product_info} > 0'.\")\n",
    "            else:\n",
    "                print(f\"Warning: Column '{discount_column_from_product_info}' not found. 'is_on_sale' defaulting to False.\")\n",
    "                merged_df['is_on_sale'] = False\n",
    "            \n",
    "            # *** NEW FEATURE ENGINEERING: Price relative to category average ***\n",
    "            PRODUCT_PRICE_COL_MERGED = 'actual_price_usd_product' # From skincare_product_info, suffixed\n",
    "            PRODUCT_CATEGORY_COL_MERGED = 'primary_category_product' # From skincare_product_info, suffixed\n",
    "            \n",
    "            if PRODUCT_PRICE_COL_MERGED in merged_df.columns and PRODUCT_CATEGORY_COL_MERGED in merged_df.columns:\n",
    "                # Ensure price is numeric (already partially handled in your feature engineering cell, but good to ensure here too)\n",
    "                merged_df[PRODUCT_PRICE_COL_MERGED] = pd.to_numeric(merged_df[PRODUCT_PRICE_COL_MERGED], errors='coerce')\n",
    "                \n",
    "                # Calculate average price per category\n",
    "                print(f\"\\nCalculating average price for each '{PRODUCT_CATEGORY_COL_MERGED}' using '{PRODUCT_PRICE_COL_MERGED}'.\")\n",
    "                # Use transform to get a series of the same length as merged_df for easy division\n",
    "                category_avg_prices = merged_df.groupby(PRODUCT_CATEGORY_COL_MERGED)[PRODUCT_PRICE_COL_MERGED].transform('mean')\n",
    "                \n",
    "                # Calculate the price ratio feature\n",
    "                merged_df['price_ratio_to_category_avg'] = merged_df[PRODUCT_PRICE_COL_MERGED] / category_avg_prices\n",
    "                \n",
    "                # Handle potential inf values (if category_avg_price was 0) and NaNs by replacing inf with NaN,\n",
    "                # then deciding on imputation (e.g., 1.0 if product price matches category average, or let imputer handle later)\n",
    "                merged_df['price_ratio_to_category_avg'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                # Optional: fill NaNs with 1, assuming if data is missing, price is \"average\" for its category. \n",
    "                # Otherwise, the imputer in the pipeline will handle it.\n",
    "                # merged_df['price_ratio_to_category_avg'].fillna(1.0, inplace=True) \n",
    "                print(\"Created 'price_ratio_to_category_avg' feature.\")\n",
    "                print(\"Sample of new price ratio feature (first 5 with actual price and category for context):\")\n",
    "                print(merged_df[[PRODUCT_CATEGORY_COL_MERGED, PRODUCT_PRICE_COL_MERGED, 'price_ratio_to_category_avg']].head())\n",
    "            else:\n",
    "                print(f\"Warning: Columns '{PRODUCT_PRICE_COL_MERGED}' or '{PRODUCT_CATEGORY_COL_MERGED}' not found. Cannot create 'price_ratio_to_category_avg'.\")\n",
    "                merged_df['price_ratio_to_category_avg'] = np.nan # Create column as NaN\n",
    "\n",
    "            print(\"\\nMerged Data Sample (selected columns):\")\n",
    "            display_cols_sample = [client_id_col_in_reviews, 'product_id', 'is_on_sale']\n",
    "            price_col_for_sample = PRODUCT_PRICE_COL_MERGED if PRODUCT_PRICE_COL_MERGED in merged_df.columns else ('actual_price_usd_review' if 'actual_price_usd_review' in merged_df.columns else None)\n",
    "            if price_col_for_sample:\n",
    "                display_cols_sample.insert(2, price_col_for_sample)\n",
    "            if 'price_ratio_to_category_avg' in merged_df.columns:\n",
    "                 display_cols_sample.append('price_ratio_to_category_avg')\n",
    "            print(merged_df[display_cols_sample].head())\n",
    "    else:\n",
    "        print(\"Halting merge due to missing 'product_id' columns in one or both dataframes.\")\n",
    "        merged_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d37e29",
   "metadata": {},
   "source": [
    "## Cell 4: Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if 'merged_df' not in locals() or not isinstance(merged_df, pd.DataFrame) or merged_df.empty:\n",
    "    print(\"merged_df is not available or empty. Skipping Feature Engineering & Selection.\")\n",
    "    customer_df = pd.DataFrame() \n",
    "    customer_df_for_clustering = pd.DataFrame() \n",
    "else:\n",
    "    print(\"\\n--- Starting Feature Engineering (from merged_df) ---\")\n",
    "    \n",
    "    CLIENT_ID_COLUMN_NAME = 'author_id'\n",
    "\n",
    "    if CLIENT_ID_COLUMN_NAME not in merged_df.columns:\n",
    "        print(f\"CRITICAL ERROR: Client ID column '{CLIENT_ID_COLUMN_NAME}' not found in merged_df.\")\n",
    "        customer_df = pd.DataFrame()\n",
    "        customer_df_for_clustering = pd.DataFrame()\n",
    "    else:\n",
    "        merged_df.dropna(subset=[CLIENT_ID_COLUMN_NAME], inplace=True)\n",
    "        merged_df[CLIENT_ID_COLUMN_NAME] = pd.to_numeric(merged_df[CLIENT_ID_COLUMN_NAME], errors='coerce')\n",
    "        merged_df.dropna(subset=[CLIENT_ID_COLUMN_NAME], inplace=True) \n",
    "        merged_df[CLIENT_ID_COLUMN_NAME] = merged_df[CLIENT_ID_COLUMN_NAME].astype(np.int64)\n",
    "\n",
    "        REVIEW_RATING_COL = 'rating_review' # Assuming 'rating_review' from your sample data; was 'user_rating'\n",
    "        REVIEW_RECOMMENDED_COL = 'is_recommended' # This seems to be from the original reviews_df directly, no suffix\n",
    "        \n",
    "        PRODUCT_PRICE_COL = 'actual_price_usd_product' \n",
    "        PRODUCT_RATING_COL = 'rating_product'  # Assuming 'rating_product' from your sample; was 'rating'\n",
    "        PRODUCT_LOVES_COL = 'loves_count_product'      \n",
    "        PRODUCT_BRAND_COL = 'brand_name_product'      \n",
    "        PRODUCT_CATEGORY_COL = 'primary_category_product' \n",
    "        PRODUCT_LIMITED_EDITION_COL = 'limited_edition_product' \n",
    "        PRODUCT_NEW_COL = 'new_product'               \n",
    "        PRODUCT_ONLINE_ONLY_COL = 'online_only_product' \n",
    "        PRODUCT_SEPHORA_EXCLUSIVE_COL = 'sephora_exclusive_product' \n",
    "        PRODUCT_IS_ON_SALE_COL = 'is_on_sale'\n",
    "        \n",
    "        # *** NEW: Define column name for the price ratio feature ***\n",
    "        PRICE_RATIO_VS_CATEGORY_COL = 'price_ratio_to_category_avg' # Created in the previous cell\n",
    "\n",
    "        print(\"\\n--- Confirming availability and Pre-cleaning Numeric Source Columns ---\")\n",
    "        source_columns_map = {\n",
    "            REVIEW_RATING_COL: \"reviews.csv ('rating_review')\",\n",
    "            REVIEW_RECOMMENDED_COL: \"reviews.csv ('is_recommended')\",\n",
    "            PRODUCT_PRICE_COL: \"skincare_product_info.csv ('actual_price_usd_product')\",\n",
    "            PRODUCT_RATING_COL: \"skincare_product_info.csv ('rating_product')\",\n",
    "            PRODUCT_LOVES_COL: \"skincare_product_info.csv ('loves_count_product')\",\n",
    "            PRODUCT_BRAND_COL: \"skincare_product_info.csv ('brand_name_product')\",\n",
    "            PRODUCT_CATEGORY_COL: \"skincare_product_info.csv ('primary_category_product')\",\n",
    "            PRODUCT_LIMITED_EDITION_COL: \"skincare_product_info.csv ('limited_edition_product')\",\n",
    "            PRODUCT_NEW_COL: \"skincare_product_info.csv ('new_product')\",\n",
    "            PRODUCT_ONLINE_ONLY_COL: \"skincare_product_info.csv ('online_only_product')\",\n",
    "            PRODUCT_SEPHORA_EXCLUSIVE_COL: \"skincare_product_info.csv ('sephora_exclusive_product')\",\n",
    "            PRODUCT_IS_ON_SALE_COL: \"Engineered ('is_on_sale')\",\n",
    "            PRICE_RATIO_VS_CATEGORY_COL: \"Engineered ('price_ratio_to_category_avg')\" # *** NEW ***\n",
    "        }\n",
    "        \n",
    "        all_source_cols_found = True\n",
    "        for col_name, col_source_desc in source_columns_map.items():\n",
    "            if col_name not in merged_df.columns:\n",
    "                print(f\"Warning: Expected source column '{col_name}' (for {col_source_desc}) NOT FOUND in merged_df. Related features will be NaN or cause errors.\")\n",
    "                all_source_cols_found = False\n",
    "            else:\n",
    "                if col_name == PRODUCT_PRICE_COL:\n",
    "                    print(f\"Cleaning and converting '{col_name}' to numeric...\")\n",
    "                    merged_df[col_name] = merged_df[col_name].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n",
    "                    merged_df[col_name] = pd.to_numeric(merged_df[col_name], errors='coerce')\n",
    "                elif col_name in [REVIEW_RATING_COL, PRODUCT_RATING_COL, PRODUCT_LOVES_COL, REVIEW_RECOMMENDED_COL, PRICE_RATIO_VS_CATEGORY_COL]: # *** Added PRICE_RATIO_VS_CATEGORY_COL ***\n",
    "                    print(f\"Converting '{col_name}' to numeric...\")\n",
    "                    merged_df[col_name] = pd.to_numeric(merged_df[col_name], errors='coerce')\n",
    "\n",
    "\n",
    "        if all_source_cols_found:\n",
    "            print(\"All key source columns for feature engineering appear to be available and have been pre-processed in merged_df.\")\n",
    "        print(\"--- End of source column confirmation and pre-cleaning ---\\n\")\n",
    "        \n",
    "        print(\"Calculating customer features using groupby().agg()...\")\n",
    "        \n",
    "        agg_functions = {}\n",
    "        if REVIEW_RATING_COL in merged_df.columns and pd.api.types.is_numeric_dtype(merged_df[REVIEW_RATING_COL]):\n",
    "            agg_functions['avg_rating_given'] = (REVIEW_RATING_COL, 'mean')\n",
    "        \n",
    "        if REVIEW_RECOMMENDED_COL in merged_df.columns and pd.api.types.is_numeric_dtype(merged_df[REVIEW_RECOMMENDED_COL]):\n",
    "            agg_functions['prop_recommended'] = (REVIEW_RECOMMENDED_COL, 'mean')\n",
    "        \n",
    "        agg_functions['num_reviews'] = (CLIENT_ID_COLUMN_NAME, 'count')\n",
    "\n",
    "        if PRODUCT_PRICE_COL in merged_df.columns and pd.api.types.is_numeric_dtype(merged_df[PRODUCT_PRICE_COL]):\n",
    "            agg_functions['avg_price_reviewed'] = (PRODUCT_PRICE_COL, 'mean')\n",
    "            agg_functions['total_value_reviewed'] = (PRODUCT_PRICE_COL, 'sum')\n",
    "        \n",
    "        if PRODUCT_RATING_COL in merged_df.columns and pd.api.types.is_numeric_dtype(merged_df[PRODUCT_RATING_COL]):\n",
    "            agg_functions['avg_product_rating_reviewed'] = (PRODUCT_RATING_COL, 'mean')\n",
    "        \n",
    "        if PRODUCT_LOVES_COL in merged_df.columns and pd.api.types.is_numeric_dtype(merged_df[PRODUCT_LOVES_COL]):\n",
    "            agg_functions['avg_loves_count_reviewed'] = (PRODUCT_LOVES_COL, 'mean')\n",
    "        \n",
    "        if PRODUCT_BRAND_COL in merged_df.columns: \n",
    "            agg_functions['num_unique_brands_reviewed'] = (PRODUCT_BRAND_COL, 'nunique')\n",
    "        if PRODUCT_CATEGORY_COL in merged_df.columns: \n",
    "            agg_functions['num_unique_categories_reviewed'] = (PRODUCT_CATEGORY_COL, 'nunique')\n",
    "        \n",
    "        # *** NEW: Add aggregation for the price ratio feature ***\n",
    "        if PRICE_RATIO_VS_CATEGORY_COL in merged_df.columns and pd.api.types.is_numeric_dtype(merged_df[PRICE_RATIO_VS_CATEGORY_COL]):\n",
    "            agg_functions['avg_price_ratio_vs_category'] = (PRICE_RATIO_VS_CATEGORY_COL, 'mean')\n",
    "        else:\n",
    "            print(f\"Warning: '{PRICE_RATIO_VS_CATEGORY_COL}' not available or not numeric in merged_df. Cannot create 'avg_price_ratio_vs_category' feature.\")\n",
    "\n",
    "        bool_like_cols = {\n",
    "            PRODUCT_LIMITED_EDITION_COL: 'prop_limited_edition',\n",
    "            PRODUCT_NEW_COL: 'prop_new_product',\n",
    "            PRODUCT_ONLINE_ONLY_COL: 'prop_online_only',\n",
    "            PRODUCT_SEPHORA_EXCLUSIVE_COL: 'prop_sephora_exclusive',\n",
    "            PRODUCT_IS_ON_SALE_COL: 'prop_on_sale_reviewed'\n",
    "        }\n",
    "        for col, new_name in bool_like_cols.items():\n",
    "            if col in merged_df.columns:\n",
    "                 agg_functions[new_name] = (col, lambda x: pd.to_numeric(x, errors='coerce').mean())\n",
    "\n",
    "\n",
    "        if not agg_functions: \n",
    "            print(\"Error: No valid aggregation functions could be defined. Cannot create customer_df robustly.\")\n",
    "            customer_df = pd.DataFrame()\n",
    "            customer_df_for_clustering = pd.DataFrame()\n",
    "        else:\n",
    "            print(f\"Attempting aggregation with functions: {list(agg_functions.keys())}\") # Show all keys\n",
    "            try:\n",
    "                customer_df = merged_df.groupby(CLIENT_ID_COLUMN_NAME).agg(\n",
    "                    **agg_functions\n",
    "                ).reset_index()\n",
    "                customer_df.rename(columns={CLIENT_ID_COLUMN_NAME: 'client_id'}, inplace=True)\n",
    "                customer_df.set_index('client_id', inplace=True)\n",
    "                print(\"Customer features calculated.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during groupby().agg(): {e}\")\n",
    "                cols_in_agg = [val[0] for val in agg_functions.values() if isinstance(val, tuple)]\n",
    "                if cols_in_agg:\n",
    "                    # print(f\"Dtypes in merged_df for columns used in aggregation: \\n{merged_df[list(set(cols_in_agg))].info()}\")\n",
    "                    pass # Avoid overly verbose output\n",
    "                customer_df = pd.DataFrame() \n",
    "                customer_df_for_clustering = pd.DataFrame() \n",
    "\n",
    "        if not customer_df.empty:\n",
    "            print(f\"\\nCustomer features DataFrame created. Shape: {customer_df.shape}\")\n",
    "            print(\"Customer DataFrame sample:\")\n",
    "            print(customer_df.head())\n",
    "            \n",
    "            print(\"\\nSelecting all available engineered numeric features for clustering as a starting point.\")\n",
    "            existing_numeric_cols = [col for col in customer_df.columns if pd.api.types.is_numeric_dtype(customer_df[col])]\n",
    "\n",
    "            if not existing_numeric_cols:\n",
    "                 print(\"Error: No numeric features available in customer_df for clustering.\")\n",
    "                 customer_df_for_clustering = pd.DataFrame()\n",
    "            else:\n",
    "                customer_df_for_clustering = customer_df[existing_numeric_cols].copy()\n",
    "                print(f\"Using these features for clustering: {customer_df_for_clustering.columns.tolist()}\")\n",
    "                print(\"\\nSample of data selected for clustering (customer_df_for_clustering):\")\n",
    "                print(customer_df_for_clustering.head())\n",
    "                print(\"\\nNaNs per selected feature (before imputation for clustering):\")\n",
    "                print(customer_df_for_clustering.isnull().sum())\n",
    "        else:\n",
    "            print(\"Customer DataFrame is empty after feature engineering attempt.\")\n",
    "            customer_df_for_clustering = pd.DataFrame() \n",
    "            \n",
    "if 'customer_df' not in locals():\n",
    "    customer_df = pd.DataFrame()\n",
    "if 'customer_df_for_clustering' not in locals():\n",
    "    customer_df_for_clustering = pd.DataFrame()\n",
    "\n",
    "print(\"\\n--- Feature Engineering & Selection Cell Finished ---\")\n",
    "print(f\"Shape of customer_df: {customer_df.shape if isinstance(customer_df, pd.DataFrame) else 'Not a DataFrame'}\")\n",
    "print(f\"Shape of customer_df_for_clustering: {customer_df_for_clustering.shape if isinstance(customer_df_for_clustering, pd.DataFrame) else 'Not a DataFrame'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c5f47",
   "metadata": {},
   "source": [
    "## Cell 5: Prepare for Clustering (Imputation & Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23628e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'customer_df_for_clustering' not in locals() or customer_df_for_clustering.empty:\n",
    "    print(\"Data for clustering is not available. Skipping preparation.\")\n",
    "    processed_customer_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"\\n--- Preparing Data for Clustering (Imputation & Scaling) ---\")\n",
    "    \n",
    "    features_to_process = customer_df_for_clustering.columns.tolist()\n",
    "    \n",
    "    numerical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')), # Impute NaNs\n",
    "        ('scaler', StandardScaler())                   # Scale features\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_pipeline, features_to_process)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        processed_features_array = preprocessor.fit_transform(customer_df_for_clustering)\n",
    "        processed_customer_df = pd.DataFrame(processed_features_array, \n",
    "                                             columns=features_to_process, \n",
    "                                             index=customer_df_for_clustering.index)\n",
    "        \n",
    "        print(\"\\nProcessed (scaled and imputed) customer features for clustering (sample):\")\n",
    "        print(processed_customer_df.head())\n",
    "        \n",
    "        if processed_customer_df.isnull().sum().any():\n",
    "            print(\"\\nWARNING: NaNs found in processed_customer_df AFTER imputation. Check pipeline/data.\")\n",
    "            print(processed_customer_df.isnull().sum())\n",
    "        else:\n",
    "            print(\"\\nNo NaNs in processed_customer_df. Ready for clustering.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing for clustering: {e}\")\n",
    "        processed_customer_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada1dac",
   "metadata": {},
   "source": [
    "## Cell 6: Determine Optimal Number of Clusters (Silhouette Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a69fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go # Import Plotly\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assume processed_customer_df is loaded and preprocessed\n",
    "# For testing, let's create a dummy processed_customer_df if it doesn't exist\n",
    "if 'processed_customer_df' not in locals() or not isinstance(processed_customer_df, pd.DataFrame):\n",
    "    print(\"Dummy processed_customer_df created for testing.\")\n",
    "    num_samples_dummy = 100 \n",
    "    num_features_dummy = 5\n",
    "    if num_samples_dummy > 50:\n",
    "         processed_customer_df = pd.DataFrame(np.random.rand(num_samples_dummy, num_features_dummy))\n",
    "    else:\n",
    "         processed_customer_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "if 'processed_customer_df' not in locals() or not isinstance(processed_customer_df, pd.DataFrame) or processed_customer_df.empty:\n",
    "    print(\"Processed customer DataFrame is empty or not a DataFrame. Skipping Silhouette analysis.\")\n",
    "else:\n",
    "    print(\"\\n--- Determining Optimal K using Silhouette Method ---\")\n",
    "    print(f\"Shape of processed_customer_df: {processed_customer_df.shape}\")\n",
    "\n",
    "    silhouette_scores = []\n",
    "    k_range_start = 2\n",
    "    k_range_end = 30 \n",
    "    k_range_step = 2  \n",
    "    k_range = range(k_range_start, k_range_end, k_range_step)\n",
    "\n",
    "    current_max_k = max(k_range) if list(k_range) else 0\n",
    "\n",
    "    if current_max_k > 0 and processed_customer_df.shape[0] < current_max_k:\n",
    "        print(f\"Warning: Number of samples ({processed_customer_df.shape[0]}) is less than the maximum K being tested ({current_max_k}).\")\n",
    "        new_upper_k_limit = processed_customer_df.shape[0] -1\n",
    "        if new_upper_k_limit < k_range_start:\n",
    "            k_range = [] \n",
    "        else:\n",
    "            k_range = range(k_range_start, new_upper_k_limit + 1, k_range_step)\n",
    "        current_max_k = max(k_range) if list(k_range) else 0\n",
    "        print(f\"Adjusted k_range due to small sample size: {list(k_range)}\")\n",
    "\n",
    "    optimal_k_silhouette = 3 # Default K, will be updated if analysis runs\n",
    "\n",
    "    if not list(k_range):\n",
    "        print(\"Not enough data points or k_range is empty/too small to perform Silhouette analysis. Setting K=3 as default for next step if applicable.\")\n",
    "        k_range_for_plot = []\n",
    "        # optimal_k_silhouette is already 3\n",
    "    else:\n",
    "        k_range_for_plot = list(k_range)\n",
    "        print(f\"Testing K values in range: {k_range_for_plot}\")\n",
    "        \n",
    "        silhouette_sample_size = None\n",
    "        if processed_customer_df.shape[0] > 5000: \n",
    "            silhouette_sample_size = 5000 \n",
    "            print(f\"Using sample_size={silhouette_sample_size} for Silhouette score calculation due to large dataset.\")\n",
    "\n",
    "        for k_val in k_range_for_plot:\n",
    "            if k_val >= processed_customer_df.shape[0]:\n",
    "                print(f\"Skipping K={k_val} as it's >= number of samples ({processed_customer_df.shape[0]}). Appending NaN.\")\n",
    "                silhouette_scores.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nProcessing K={k_val}...\")\n",
    "            try:\n",
    "                print(f\"  Fitting KMeans for K={k_val} (n_init=3)...\")\n",
    "                kmeans_model = KMeans(n_clusters=k_val, random_state=42, n_init=3, algorithm='lloyd')\n",
    "                cluster_labels_temp = kmeans_model.fit_predict(processed_customer_df)\n",
    "                \n",
    "                num_unique_labels = len(np.unique(cluster_labels_temp))\n",
    "                if num_unique_labels > 1 and processed_customer_df.shape[0] > k_val:\n",
    "                    print(f\"  Calculating Silhouette Score for K={k_val} (unique labels: {num_unique_labels})...\")\n",
    "                    score = silhouette_score(processed_customer_df, cluster_labels_temp, sample_size=silhouette_sample_size, random_state=42)\n",
    "                    silhouette_scores.append(score)\n",
    "                    print(f\"  Silhouette Score for K={k_val}: {score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  Could not calculate Silhouette Score for K={k_val}. Conditions not met:\")\n",
    "                    print(f\"    Unique clusters found: {num_unique_labels} (need > 1)\")\n",
    "                    print(f\"    Samples ({processed_customer_df.shape[0]}) vs K ({k_val}) (need samples > K)\")\n",
    "                    silhouette_scores.append(np.nan)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error calculating Silhouette for K={k_val}: {e}. Appending NaN.\")\n",
    "                silhouette_scores.append(np.nan)\n",
    "\n",
    "    # --- Plotting with Plotly ---\n",
    "    if k_range_for_plot and any(not np.isnan(s) for s in silhouette_scores if isinstance(s, float)):\n",
    "        valid_k_for_plot = [k_range_for_plot[i] for i, s in enumerate(silhouette_scores) if not np.isnan(s)]\n",
    "        valid_scores_for_plot = [s for s in silhouette_scores if not np.isnan(s)]\n",
    "\n",
    "        if valid_k_for_plot:\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=valid_k_for_plot,\n",
    "                y=valid_scores_for_plot,\n",
    "                mode='lines+markers',\n",
    "                marker=dict(color='red', size=8), # Red markers\n",
    "                line=dict(color='red', width=2),   # Red line\n",
    "                name='Silhouette Score'\n",
    "            ))\n",
    "\n",
    "            fig.update_layout(\n",
    "                title='Silhouette Scores for Optimal K',\n",
    "                xaxis_title='Number of Clusters (K)',\n",
    "                yaxis_title='Silhouette Score',\n",
    "                xaxis=dict(tickmode='array', tickvals=valid_k_for_plot, showgrid=True, gridcolor='LightGrey'),\n",
    "                yaxis=dict(showgrid=True, gridcolor='LightGrey'),\n",
    "                plot_bgcolor='white',\n",
    "                height=600,\n",
    "                width=800,\n",
    "                font=dict(family='Arial', size=12)\n",
    "            )\n",
    "            fig.show()\n",
    "            \n",
    "            if valid_scores_for_plot:\n",
    "                optimal_k_silhouette = valid_k_for_plot[np.argmax(valid_scores_for_plot)]\n",
    "                print(f\"Recommended K based on highest Silhouette Score: {optimal_k_silhouette}\")\n",
    "            # else optimal_k_silhouette remains 3 (default)\n",
    "        else:\n",
    "            print(\"No valid data to plot Silhouette scores (all were NaN or k_range was empty).\")\n",
    "            # optimal_k_silhouette remains 3 (default)\n",
    "\n",
    "    elif k_range_for_plot:\n",
    "        print(\"Silhouette analysis completed, but all scores were NaN. Cannot plot or recommend K. Setting K=3 as default.\")\n",
    "        # optimal_k_silhouette remains 3 (default)\n",
    "    else: # k_range_for_plot was empty from the start\n",
    "        print(\"Silhouette analysis skipped due to insufficient data or k_range size. Optimal K not determined by Silhouette.\")\n",
    "        # optimal_k_silhouette remains 3 (default)\n",
    "\n",
    "    # Ensure optimal_k_silhouette has a value for subsequent cells\n",
    "    # This was already handled by initializing to 3 and updating if possible.\n",
    "    print(f\"Final optimal_k_silhouette to be used: {optimal_k_silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3dc92",
   "metadata": {},
   "source": [
    "## Cell 7: K-Means Clustering\n",
    "Based on the Silhouette plot and scores from the previous step (and potentially other domain knowledge), choose a value for `K_CLUSTERS`. The K with the highest Silhouette score is often a good choice, but also consider the interpretability of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'processed_customer_df' not in locals() or processed_customer_df.empty:\n",
    "    print(\"Processed customer DataFrame is empty. Skipping K-Means clustering.\")\n",
    "    customer_df_final_with_clusters = pd.DataFrame() \n",
    "else:\n",
    "    print(\"\\n--- Performing K-Means Clustering ---\")\n",
    "    # !!! SET THIS VALUE BASED ON YOUR SILHOUETTE ANALYSIS (and Elbow if you run it) !!!\n",
    "    K_CLUSTERS = 12 # Example value, ADJUST AS NEEDED (e.g., optimal_k_silhouette if defined)\n",
    "    print(f\"Using K = {K_CLUSTERS} clusters.\")\n",
    "\n",
    "    if K_CLUSTERS <= 1 or K_CLUSTERS > processed_customer_df.shape[0]:\n",
    "        print(f\"Error: Invalid K_CLUSTERS value ({K_CLUSTERS}) for dataset size {processed_customer_df.shape[0]}. Cannot proceed.\")\n",
    "        customer_df_final_with_clusters = pd.DataFrame()\n",
    "    else:\n",
    "        kmeans_final_model = KMeans(n_clusters=K_CLUSTERS, random_state=42, n_init='auto')\n",
    "        cluster_labels = kmeans_final_model.fit_predict(processed_customer_df)\n",
    "        \n",
    "        # Use customer_df_for_clustering (unscaled, selected features) for easier interpretation of cluster means\n",
    "        customer_df_interpretable = customer_df_for_clustering.copy()\n",
    "        \n",
    "        if customer_df_interpretable.isnull().values.any():\n",
    "            print(\"Imputing NaNs in customer_df_interpretable for cluster analysis (using median)...\")\n",
    "            for col in customer_df_interpretable.columns:\n",
    "                if customer_df_interpretable[col].isnull().any():\n",
    "                    median_val = customer_df_interpretable[col].median()\n",
    "                    customer_df_interpretable[col].fillna(median_val, inplace=True)\n",
    "        \n",
    "        customer_df_final_with_clusters = customer_df_interpretable.copy()\n",
    "        customer_df_final_with_clusters['cluster_id'] = cluster_labels\n",
    "        \n",
    "        print(f\"\\nCustomers with assigned cluster IDs (sample):\")\n",
    "        display_cols = ['cluster_id'] + customer_df_for_clustering.columns.tolist()[:min(3, len(customer_df_for_clustering.columns))]\n",
    "        print(customer_df_final_with_clusters[display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18df9c0",
   "metadata": {},
   "source": [
    "## Cell 8: Analyze Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e74e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "if 'customer_df_final_with_clusters' not in locals() or not isinstance(customer_df_final_with_clusters, pd.DataFrame) or customer_df_final_with_clusters.empty:\n",
    "    print(\"Final customer DataFrame with clusters is not available or not a DataFrame. Skipping cluster analysis.\")\n",
    "else:\n",
    "    print(\"\\n--- Analyzing Cluster Characteristics ---\")\n",
    "    \n",
    "    if 'cluster_id' not in customer_df_final_with_clusters.columns:\n",
    "        print(\"Error: 'cluster_id' column not found in customer_df_final_with_clusters. Skipping analysis.\")\n",
    "    else:\n",
    "        numeric_cols = customer_df_final_with_clusters.select_dtypes(include=np.number).columns.tolist()\n",
    "        if 'cluster_id' in numeric_cols:\n",
    "            numeric_cols.remove('cluster_id')\n",
    "        \n",
    "        if not numeric_cols:\n",
    "            print(\"Error: No numeric columns found for cluster summary (excluding cluster_id).\")\n",
    "            cluster_summary = pd.DataFrame()\n",
    "        else:\n",
    "            cluster_summary = customer_df_final_with_clusters.groupby('cluster_id')[numeric_cols].mean()\n",
    "\n",
    "        cluster_sizes = customer_df_final_with_clusters['cluster_id'].value_counts().sort_index()\n",
    "        cluster_summary['cluster_size'] = cluster_sizes\n",
    "        \n",
    "        print(\"\\nCluster Summary (Mean feature values and size per cluster):\")\n",
    "        with pd.option_context('display.float_format', '{:,.2f}'.format, 'display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "            print(cluster_summary)\n",
    "        \n",
    "        # --- REWRITTEN BAR PLOTS USING PLOTLY GO (Red and Black) ---\n",
    "        features_for_bar_plot = ['avg_price_reviewed', 'avg_rating_given'] \n",
    "        available_features_for_barplot = [col for col in features_for_bar_plot if col in cluster_summary.columns]\n",
    "\n",
    "        if len(available_features_for_barplot) > 0:\n",
    "            print(f\"\\n--- Generating Plotly Bar Plots for: {available_features_for_barplot} ---\")\n",
    "            num_bar_plots = len(available_features_for_barplot)\n",
    "            \n",
    "            fig_bar_plotly = make_subplots(\n",
    "                rows=1, cols=num_bar_plots,\n",
    "                subplot_titles=[f'Mean {feature} per Cluster' for feature in available_features_for_barplot]\n",
    "            )\n",
    "\n",
    "            bar_plot_colors = ['red', 'black'] # Define color cycle for bar plots\n",
    "\n",
    "            for i, feature_to_plot in enumerate(available_features_for_barplot):\n",
    "                current_bar_color = bar_plot_colors[i % len(bar_plot_colors)] # Cycle through red and black\n",
    "                fig_bar_plotly.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=cluster_summary.index, \n",
    "                        y=cluster_summary[feature_to_plot],\n",
    "                        text=cluster_summary[feature_to_plot].apply(lambda x: f'{x:.2f}'),\n",
    "                        textposition='auto',\n",
    "                        name=feature_to_plot,\n",
    "                        marker_color=current_bar_color, # Apply red or black\n",
    "                        hovertemplate=(\n",
    "                            f\"<b>Cluster ID: %{{x}}</b><br>\"\n",
    "                            f\"Mean {feature_to_plot}: %{{y:.2f}}\"\n",
    "                            \"<extra></extra>\"\n",
    "                        )\n",
    "                    ),\n",
    "                    row=1, col=i+1\n",
    "                )\n",
    "                fig_bar_plotly.update_xaxes(title_text=\"Cluster ID\", row=1, col=i+1, type='category')\n",
    "                fig_bar_plotly.update_yaxes(title_text=f'Mean {feature_to_plot}', row=1, col=i+1)\n",
    "\n",
    "            fig_bar_plotly.update_layout(\n",
    "                height=500,\n",
    "                showlegend=False, \n",
    "                title_text=\"Cluster Characteristics (Bar Plots)\",\n",
    "                plot_bgcolor='white',\n",
    "                font=dict(family='Arial', size=12),\n",
    "                margin=dict(t=80, b=60, l=60, r=60)\n",
    "            )\n",
    "            fig_bar_plotly.update_xaxes(showgrid=False)\n",
    "            fig_bar_plotly.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "            \n",
    "            fig_bar_plotly.show()\n",
    "        else:\n",
    "            print(f\"\\nNote: No features from '{features_for_bar_plot}' available in cluster_summary for Plotly bar plotting.\")\n",
    "\n",
    "        # --- PLOTLY GO SCATTER PLOT (Red and Black based on condition) ---\n",
    "        print(\"\\n--- Generating Plotly Scatter Plot for Cluster Analysis ---\")\n",
    "        \n",
    "        x_feature_scatter = 'avg_price_reviewed'\n",
    "        y_feature_scatter = 'avg_price_ratio_vs_category'\n",
    "        size_feature_scatter = 'cluster_size'\n",
    "\n",
    "        if x_feature_scatter in cluster_summary.columns and \\\n",
    "           y_feature_scatter in cluster_summary.columns and \\\n",
    "           size_feature_scatter in cluster_summary.columns:\n",
    "\n",
    "            # Determine colors for scatter plot based on y_feature_scatter's relation to 1.0\n",
    "            scatter_marker_colors = np.where(cluster_summary[y_feature_scatter] > 1.0, 'red', 'black')\n",
    "\n",
    "            fig_scatter = go.Figure()\n",
    "            min_marker_size = 5\n",
    "            scaled_sizes = min_marker_size + np.sqrt(cluster_summary[size_feature_scatter]) * 2 \n",
    "            scaled_sizes = np.clip(scaled_sizes, min_marker_size, 40)\n",
    "\n",
    "            fig_scatter.add_trace(go.Scatter(\n",
    "                x=cluster_summary[x_feature_scatter],\n",
    "                y=cluster_summary[y_feature_scatter],\n",
    "                mode='markers+text',\n",
    "                marker=dict(\n",
    "                    size=scaled_sizes,\n",
    "                    color=scatter_marker_colors, # Apply conditional red/black colors\n",
    "                    # colorscale='Viridis', # Removed\n",
    "                    # showscale=True, # Removed\n",
    "                    line=dict(width=1, color='DarkSlateGrey') # Marker outline\n",
    "                ),\n",
    "                text=cluster_summary.index.astype(str), \n",
    "                textposition=\"top center\",\n",
    "                hovertemplate=(\n",
    "                    f\"<b>Cluster ID: %{{text}}</b><br>\"\n",
    "                    f\"{x_feature_scatter}: %{{x:.2f}}<br>\"\n",
    "                    f\"{y_feature_scatter}: %{{y:.2f}}<br>\"\n",
    "                    f\"{size_feature_scatter}: %{{customdata[0]}}\"\n",
    "                    \"<extra></extra>\"\n",
    "                ),\n",
    "                customdata=cluster_summary[[size_feature_scatter]] \n",
    "            ))\n",
    "\n",
    "            median_x = cluster_summary[x_feature_scatter].median()\n",
    "            median_y = cluster_summary[y_feature_scatter].median()\n",
    "            \n",
    "            fig_scatter.add_hline(y=1.0, line_dash=\"dash\", line_color=\"grey\",\n",
    "                                  annotation_text=\"Ratio = 1 (Category Avg Price)\", annotation_position=\"bottom right\")\n",
    "            fig_scatter.add_hline(y=median_y, line_dash=\"dot\", line_color=\"lightgrey\",\n",
    "                                  annotation_text=f\"Median Ratio: {median_y:.2f}\", annotation_position=\"top right\")\n",
    "            fig_scatter.add_vline(x=median_x, line_dash=\"dot\", line_color=\"lightgrey\",\n",
    "                                  annotation_text=f\"Median Avg Price: {median_x:.2f}\", annotation_position=\"bottom right\")\n",
    "            \n",
    "            y_range_scatter = [cluster_summary[y_feature_scatter].min() * 0.9, cluster_summary[y_feature_scatter].max() * 1.1]\n",
    "            if len(cluster_summary[y_feature_scatter].unique()) == 1: \n",
    "                y_range_scatter = [y_range_scatter[0] - 0.5, y_range_scatter[1] + 0.5 if y_range_scatter[1] > y_range_scatter[0] else y_range_scatter[0] + 0.5]\n",
    "            if y_range_scatter[0] >= y_range_scatter[1] : y_range_scatter = [y_range_scatter[0]-0.1, y_range_scatter[0]+0.1]\n",
    "\n",
    "\n",
    "            band_count = 10\n",
    "            band_shapes = []\n",
    "            if y_range_scatter[1] > y_range_scatter[0]: # Ensure valid range for bands\n",
    "                band_height = (y_range_scatter[1] - y_range_scatter[0]) / band_count\n",
    "                if band_height > 0: \n",
    "                    for i in range(0, band_count, 2):\n",
    "                        band_shapes.append(dict(\n",
    "                            type='rect', xref='paper', yref='y',\n",
    "                            x0=0, x1=1,\n",
    "                            y0=y_range_scatter[0] + i * band_height,\n",
    "                            y1=y_range_scatter[0] + (i + 1) * band_height,\n",
    "                            fillcolor='rgba(0, 0, 0, 0.05)',\n",
    "                            layer='below', line_width=0\n",
    "                        ))\n",
    "\n",
    "            fig_scatter.update_layout(\n",
    "                title=f'Cluster Analysis: {y_feature_scatter} vs. {x_feature_scatter}',\n",
    "                xaxis_title=f'Mean {x_feature_scatter} per Cluster',\n",
    "                yaxis_title=f'Mean {y_feature_scatter} per Cluster',\n",
    "                yaxis=dict(range=y_range_scatter if y_range_scatter[0] < y_range_scatter[1] else None, showgrid=False),\n",
    "                xaxis=dict(showgrid=False),\n",
    "                plot_bgcolor='white',\n",
    "                height=700,\n",
    "                font=dict(family='Arial', size=12),\n",
    "                margin=dict(t=80, b=60, l=80, r=80),\n",
    "                showlegend=False,\n",
    "                shapes=band_shapes\n",
    "            )\n",
    "            fig_scatter.show()\n",
    "        else:\n",
    "            print(f\"\\nNote: One or more features ('{x_feature_scatter}', '{y_feature_scatter}', '{size_feature_scatter}') \"\n",
    "                  \"not available in cluster_summary for Plotly scatter plot.\")\n",
    "\n",
    "\n",
    "if 'optimal_k_silhouette' not in locals():\n",
    "    print(\"\\nWarning: 'optimal_k_silhouette' not defined. Setting a default if needed by other cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2446d9",
   "metadata": {},
   "source": [
    "## Cell 9: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'customer_df_final_with_clusters' not in locals() or customer_df_final_with_clusters.empty or 'cluster_id' not in customer_df_final_with_clusters.columns:\n",
    "    print(\"Clustering was not performed or 'cluster_id' is missing. Skipping export of segments.\")\n",
    "else:\n",
    "    print(\"\\n--- Exporting Customer Segments ---\")\n",
    "    output_df = customer_df_final_with_clusters[['cluster_id']].reset_index()\n",
    "    # 'client_id' should be the name of the index after reset_index() if it was named 'client_id'\n",
    "    if 'index' in output_df.columns and customer_df_final_with_clusters.index.name == 'client_id':\n",
    "         output_df.rename(columns={'index': 'client_id'}, inplace=True)\n",
    "    elif customer_df_final_with_clusters.index.name is not None and customer_df_final_with_clusters.index.name != 'client_id':\n",
    "        output_df.rename(columns={customer_df_final_with_clusters.index.name : 'client_id'}, inplace=True)\n",
    "    \n",
    "    output_filename = \"customer_segments_refined.csv\"\n",
    "    try:\n",
    "        output_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Successfully exported refined customer segments to {output_filename}\")\n",
    "        print(\"Output CSV sample:\")\n",
    "        print(output_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e424c",
   "metadata": {},
   "source": [
    "## Customer Cluster Summaries\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 0: Contented Loyalists**\n",
    "* **Size:** 82,734 customers\n",
    "* **Summary:** Satisfied customers who stick to familiar, moderately priced, popular products and aren't swayed by trends or sales.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 1: Disappointed Critics of Popular Goods**\n",
    "* **Size:** 58,380 customers\n",
    "* **Summary:** Very critical reviewers of popular, relatively low-priced items, despite the products' general popularity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 2: Engaged Brand Explorers**\n",
    "* **Size:** 41,327 customers\n",
    "* **Summary:** Active and generally satisfied reviewers who try a good number of brands and show moderate interest in trends and sales.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 3: Online Channel Loyalists**\n",
    "* **Size:** 43,529 customers\n",
    "* **Summary:** Highly satisfied customers who predominantly purchase online-only products, often less \"viral\" items.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 4: Limited Edition & Sephora Exclusive Hunters (Sale Savvy)**\n",
    "* **Size:** 9,678 customers\n",
    "* **Summary:** Seek out limited editions and Sephora exclusives, and are the most interested in finding these items on sale.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 5: Budget Fans of Viral Hits (Sephora Exclusives)**\n",
    "* **Size:** 18,507 customers\n",
    "* **Summary:** Purchase very low-priced but extremely popular (viral) items, with a high preference for Sephora exclusives.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 6: Devoted Sephora Exclusive Fans**\n",
    "* **Size:** 63,823 customers\n",
    "* **Summary:** Highly satisfied, budget-conscious customers who overwhelmingly prefer Sephora exclusive products.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 7: New Product Aficionados (High Quality Focus)**\n",
    "* **Size:** 8,112 customers\n",
    "* **Summary:** Early adopters who focus on new products that also have high community ratings, often Sephora exclusives or online.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 8: High-Spending, Prolific Reviewing Brand Connoisseurs**\n",
    "* **Size:** 4,263 customers\n",
    "* **Summary:** Highly engaged customers who spend and review a lot, explore many brands, and show interest in new products.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 9: Premium Sale Seekers (Viral Products)**\n",
    "* **Size:** 10,492 customers\n",
    "* **Summary:** Purchase high-priced, popular items almost exclusively when they are on sale, often Sephora exclusives.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 10: Quality-Focused Budget Buyers**\n",
    "* **Size:** 123,569 customers\n",
    "* **Summary:** The largest group; very satisfied, they buy low-priced items that have excellent community ratings and are the least interested in trends or sales.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 11: Luxury Niche Loyalists**\n",
    "* **Size:** 10,212 customers\n",
    "* **Summary:** Spend the most per item by far, focusing on expensive, niche (less viral) products and are brand loyal.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 12: Sephora Exclusive Fans (Less Critical of Product Ratings)**\n",
    "* **Size:** 27,802 customers\n",
    "* **Summary:** Strongly prefer Sephora exclusives, similar to Cluster 6, but are satisfied even if those exclusives have lower overall community ratings.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cluster 13: Super Reviewers / High-Spending Trendsetters**\n",
    "* **Size:** 767 customers\n",
    "* **Summary:** A very small group of power users who review, spend, and explore brands the most, focusing on high-quality, new, and often niche items.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127616a6",
   "metadata": {},
   "source": [
    "## Cell 10: Conclusion & Next Steps (Refined Segmentation)\n",
    "\n",
    "This notebook has performed customer segmentation using data from `data/processed/`, with an emphasis on careful feature selection and Silhouette analysis for determining the number of clusters. The output is `customer_segments_refined.csv`.\n",
    "\n",
    "**Interpreting Your Clusters:**\n",
    "Carefully analyze the `Cluster Summary` table (Cell 8). The key is to identify how the average feature values differ across clusters, defining distinct customer personas. Consider not just the means, but also the relative importance of features that differentiate the groups.\n",
    "\n",
    "**Potential Next Steps & Improvements:**\n",
    "* **Iterative Feature Engineering & Selection:** The selection in Cell 4 is crucial. You might iterate on this: try different sets of features, use techniques like PCA for dimensionality reduction, or look at feature importance from predictive models (if applicable to a related task) to guide selection.\n",
    "* **Hyperparameter Tuning for K-Means:** While `random_state` ensures reproducibility, other parameters of KMeans could be explored.\n",
    "* **Alternative Clustering Algorithms:** If K-Means results are not satisfactory or its assumptions don't hold, explore DBSCAN, Agglomerative Clustering, etc.\n",
    "* **Business Validation:** Ultimately, the \"best\" segmentation is the one that is most actionable and provides meaningful insights for your business goals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
